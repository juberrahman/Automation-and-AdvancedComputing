Dask was launched in late 2014 by Matthew Rocklin with aims to bring native scalability to the Python Open Data Science Stack and overcome its single-machine restrictions. Over time, the project has grown into arguably one of the best scalable computing frameworks available for Python developers. Dask consists of several different components and APIs, which can be categorized into three layers: the scheduler, low-level APIs, and high-level APIs. What makes Dask so powerful is how these components and layers are built on top of one another. At the core is the task scheduler, which coordinates and monitors execution of computations across CPU cores and machines. These computations are represented in code as either Dask Delayed objects or Dask Futures objects (the key difference is the former are evaluated lazily —meaning they are evaluated just in time when the values are needed, while the latter are evaluated eagerly —meaning they are evaluated in real time regardless if the value is needed immediately or not). Dask's high-level APIs offer a layer of abstraction over Delayed and Futures objects. Operations on these high-level objects result in many parallel low-level operations managed by the task schedulers, which provides a seamless experience for the user. Because of this design, Dask brings four key advantages to the table:

* Dask is fully implemented in Python and natively scales NumPy, Pandas, and scikit-learn.
* Dask can be used effectively to work with both medium datasets on a single machine and large datasets on a cluster.
* Dask can be used as a general framework for parallelizing most Python objects.
* Dask has a very low configuration and maintenance overhead.

The notebooks in this folder demostrates the advantages of parallel computing using Dask for processing big data. I have used a publicly available dataset(nyc-parking-tickets) of size 8 GB (good enough for a proof of concept purpose). 
